# vim: set tabstop=8 :

Results with n=2000; 1000 train + 1000 test

Accuracy: 67%

Correlations with Z:
[array([[-0.04366275, -0.03790688,  0.04491986]]),
array([[-0.06816585,  0.03584644,  0.13537957]]),
array([[-0.00901492]])]

Corr with Y
[array([[0.29453632, 0.30194366, 0.23634865]]),
array([[ 0.35911986, -0.39440877, -0.34140826]]),
array([[0.38748893]])]

Weights
0.4196	0.9320	0.5764		1.1833	-2.3801	-1.3931
-0.5766	-1.0597	-2.0604
-0.6968	-1.3112	-0.6334

Computing bias flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.2185	0.1644	0.0008	0.2473	0.2203	0.1778	0.2565
1	0.2048	0.0955	0.2343	0.2510	0.2470	0.2546	0.2547
2	0.1762

Computing accuracy flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.0651	0.0607	0.0478	0.0517	0.0988	0.1022	0.0957
1	0.0903	0.1000	0.0687	0.1080	0.0966	0.1091	0.0978
2	0.0752

-------------------------------------------------------------------------------

Correlation with n=20,000; half train + half test

Accuracy: 66%

Correlations with Z:
array([[0.01388774, 0.00751701, 0.00202404]])
array([[0.19138929, -0.05124309,  0.16310057]])
array([[0.00499579]])

Correlations with Y:
array([[0.30492877, 0.28851882, 0.19985072]])
array([[-0.32879366, -0.3664871 , -0.33515219]])
array([[0.37897452]])

Weights
-1.9079	-0.4879	-1.0778		-1.8804	-2.5718	-1.4138
-0.8774	-0.8371	-3.4026
-1.3255	-0.8126	-0.8147

-------------------------------------------------------------------------------

Results in terms of accuracy rather than mutual info:
(n=2000; 1000 train + 1000 test)

Accuracy: 63%

Correlations with Z:
array([[-0.01053378, -0.03934354, -0.01930495]])
array([[ 0.0446038 , -0.06589311,  0.07613049]])
array([[-0.02735624]])

Correlations with Y:
array([[0.28543158, 0.29089067, 0.15181509]])
array([[-0.32045712,  0.32969093, -0.32801818]])
array([[0.32452402]])

Weights
-0.6317	-0.7838	-2.1873		-2.2715	1.4479	-2.2133
0.6223	0.8153	0.8304
-0.7554	-1.1385	-1.7987

Computing bias flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.7540	0.6930	0.5101	0.7870	0.7611	0.7090	0.7880
1	0.6340	0.7319	0.6760	0.7690	0.7420	0.7800	0.7801
2	0.6639

Computing accuracy flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.6290	0.6240	0.5800	0.6190	0.6460	0.6430	0.6500
1	0.6430	0.6400	0.6390	0.6490	0.6430	0.6420	0.6360
2       0.6500

-------------------------------------------------------------------------------

Results with Linear SVM instead of kernel SVM (n = 2000):
This was much much faster, but sometimes bias numbers were extremely low
(e.g. accuracy of 0.4!)

Accuracy: 65%

Correlations with Z:
[[0.04984276 0.04379872 0.02657785]]
[[ 0.05168868 -0.06051807  0.02865561]]
[[0.10984953]]

Correlations with Y:
[[0.34312732 0.3274439  0.1719671 ]]
[[ 0.38526566 -0.38315879  0.1865648 ]]
[[0.3787281]]

Weights
 1.0407	 0.5887	 0.9789		 1.5769	-3.3878	 0.5314
-1.2050	-0.9166	-2.7090
 0.0175	-0.0080	 0.6519
Computing bias flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.5360	0.5270	0.5010	0.5370	0.5160	0.5170	0.5130
1	0.5380	0.5330	0.4970	0.5250	0.5230	0.5390	0.5350
2	0.5560

Computing accuracy flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.6410	0.6540	0.6010	0.6500	0.6450	0.6680	0.6501
1	0.6580	0.6520	0.6031	0.6600	0.6560	0.6539	0.6570
2	0.6530

-------------------------------------------------------------------------------

Results with Nystroem kernel approximation (n=2000, acc instead of MI):
Nystroem was still about as slow as (or slower than) ordinary RBF I think, but
I didn't really check whether it scales just as poorly with the number of
samples. However, it does not seem to lose out on accuracy at all.

Accuracy: 66%

Correlations with Z:
[[0.0088829  0.03764973 0.02496752]]
[[ 0.01919808 -0.01099318 -0.00451086]]
[[0.04678855]]

Correlations with Y:
[[0.29519545 0.24581198 0.22582148]]
[[-0.34758811 -0.36725635 -0.3586299 ]]
[[0.3609034]]

Weights
-0.6263	-1.0130	-1.3218		-1.6142	-0.4433	-2.6320
-0.5809	-0.0478	-0.6735
-0.7722	-1.1589	-2.1025

Computing bias flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.7590	0.7190	0.4840	0.7940	0.7560	0.7140	0.7990
1	0.7069	0.7020	0.6880	0.7180	0.7751	0.7010	0.7811
2	0.7030

Computing accuracy flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.6290	0.6240	0.6010	0.6420	0.6690	0.6640	0.6770
1	0.6520	0.6641	0.6650	0.6529	0.6700	0.6710	0.6771
2	0.6581

-------------------------------------------------------------------------------

Results with Random Kitchen Sinks (RBFSampler) (n=2000, acc instead of MI):
This also appears to be quite slow. While remaining fairly accurate.

Accuracy: 64%

Correlations with Z:
[[ 0.011446   -0.00367913 -0.02412362]]
[[-0.00856518  0.01410455 -0.00482153]]
[[0.06559776]]

Correlations with Y:
[[0.27880593 0.24722731 0.20187171]]
[[ 0.34208772 -0.34194462  0.34507335]]
[[0.34121468]]

Weights
 0.5554	 0.5489	 1.2715		 1.1311	-2.9127	 1.3326
-1.1739	-0.5621	-1.8261
 0.7571	 0.3438	 1.4971

Computing bias flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.7600	0.7050	0.4959	0.7920	0.7600	0.7020	0.7930
1	0.6841	0.6970	0.6640	0.7079	0.6890	0.7970	0.7880
2	0.6830

Computing accuracy flows...
Layer	X1	X2	X3	X12	X13	X23	X123
0	0.6220	0.6420	0.5980	0.6340	0.6490	0.6140	0.6310
1	0.6390	0.6541	0.6550	0.6489	0.6600	0.6519	0.6570
2	0.6531

-------------------------------------------------------------------------------

Results with standard RBF SVM with thundersvm (n=2000, acc instead of MI):
This felt *slower* than ordinary sklearn RBF SVM - maybe thundersvm is really
only faster if we also use CUDA

Accuracy: 66%

Correlations with Z:
[[0.03705183 0.02910376 0.01713359]]
[[ 0.03410321 -0.03718662  0.00316512]]
[[0.06040034]]

Correlations with Y:
[[0.32724554 0.2885598  0.19383862]]
[[ 0.28879839 -0.37966797  0.35876949]]
[[0.37910756]]

Weights
 0.4639 -0.3872  0.9498          0.9470 -3.6332  1.2436
-1.8591  0.1496 -2.7139
 1.0983  0.1582  0.3841

Computing bias flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.7610  0.7340  0.5280  0.7980  0.7550  0.7330  0.7960
1       0.5250  0.6850  0.7760  0.7950  0.7890  0.7760  0.7810
2       0.6820

Computing accuracy flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.6230  0.6110  0.5730  0.6160  0.6640  0.6380  0.6570
1       0.6380  0.6580  0.6400  0.6490  0.6450  0.6520  0.6550
2       0.6570

real    9m29.049s
user    50m29.761s
sys     2m39.801s

-------------------------------------------------------------------------------

Timing results for Nystroem with linear SVM (sklearn); n=2000; acc-not-MI
All times are on my workstation. This was definitely faster.

Accuracy: 64%

Correlations with Z:
[[0.02421564 0.04657703 0.02760975]]
[[ 0.00127511 -0.01879038 -0.0467534 ]]
[[0.01670543]]

Correlations with Y:
[[0.30955278 0.26719946 0.20589025]]
[[ 0.36747556 -0.37680027 -0.38170478]]
[[0.37274234]]

Weights
 1.6737  0.8844  1.7298          2.7664 -1.3362 -0.9935
-0.4925 -0.6336 -1.3045
-0.4515 -0.2828 -1.1126

Computing bias flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.7820  0.7300  0.4910  0.7970  0.7830  0.7270  0.7970
1       0.7570  0.6750  0.6520  0.7890  0.7980  0.7060  0.8010
2       0.7190

Computing accuracy flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.6250  0.6080  0.5910  0.6320  0.6410  0.6280  0.6560
1       0.6510  0.6550  0.6480  0.6600  0.6610  0.6520  0.6520
2       0.6520

real    4m38.422s
user    26m14.490s
sys     1m27.959s

-------------------------------------------------------------------------------

Timing results for Nystroem with sklearn's SGDClassifier; n=2000; acc-not-MI
All times are on my workstation. This is much much better!

Accuracy: 62%

Correlations with Z:
[[ 0.0327498   0.03490969 -0.02294502]]
[[0.03830916 0.04186296 0.02189839]]
[[0.0067374]]

Correlations with Y:
[[0.30811813 0.25670038 0.17657098]]
[[ 0.20488004 -0.3333655   0.34173041]]
[[0.33520812]]

Weights
 0.4394  0.1246 -0.5638         -0.2522 -2.7096  2.2989
-0.7748 -1.3894 -2.1390
 0.8609  0.6078  2.0768

Computing bias flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.7830  0.7080  0.5010  0.8030  0.7800  0.7160  0.7940
1       0.7080  0.6850  0.6620  0.8070  0.8020  0.6850  0.8000
2       0.6670

Computing accuracy flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.6300  0.6110  0.5940  0.6260  0.6550  0.6250  0.6450
1       0.6050  0.6180  0.6390  0.6250  0.6450  0.6410  0.6350
2       0.6360

real    2m47.222s
user    15m43.290s
sys     0m51.973s

-------------------------------------------------------------------------------

Timing results for SGDClassifier with warm starts are identical to those above,
and since warm starts have data leakage across folds, I'm not going to do that

-------------------------------------------------------------------------------

I actually don't remember what this is... Probably the warm start timings.

Accuracy: 65%

Correlations with Z:
[[0.01769564 0.00866199 0.03042272]]
[[ 0.04181064  0.16473276 -0.01551363]]
[[-0.05543436]]

Correlations with Y:
[[0.29649704 0.28196586 0.24015779]]
[[ 0.38904162 -0.35657204 -0.10799953]]
[[0.38988392]]

Weights
 0.8343  0.4571  2.0752          2.0666 -2.3575 -0.3404
-1.8289 -1.1529 -1.0542
 0.1492 -0.1263 -0.3466

Computing bias flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.7550  0.7270  0.5260  0.7980  0.7650  0.7240  0.7880
1       0.6690  0.7830  0.5250  0.7790  0.7510  0.7840  0.7710
2       0.7370

Computing accuracy flows...
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.6090  0.6120  0.6200  0.6170  0.6590  0.6520  0.6620
1       0.6660  0.6460  0.5650  0.6480  0.6540  0.6510  0.6670
2       0.6530

real    2m47.560s
user    15m47.106s
sys     0m50.734s

-------------------------------------------------------------------------------

With full (weighted) information flow calculations: (n=2000)

Accuracy: 65%

Correlations with Z:
[[0.01199938 0.01074055 0.00823379]]
[[0.01488675 0.03939168 0.0162906 ]]
[[-0.00082149]]

Correlations with Y:
[[0.32247261 0.31013964 0.20563392]]
[[ 0.3343551  -0.38611324  0.38812356]]
[[0.38354203]]

Weights
 0.1866  0.2791  1.7136          1.6104 -2.9799  1.4776
-1.1744 -0.6672 -2.6415
 0.7832  0.6266  1.2147

Computing bias flows...
Accuracies:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.7750  0.6870  0.5090  0.8100  0.7570  0.6960  0.7920
1       0.5500  0.6710  0.7000  0.7720  0.7790  0.7660  0.7620
2       0.6570
Mutual informations:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.2308  0.1034  0.0002  0.2985  0.2000  0.1139  0.2624
1       0.0072  0.0861  0.1187  0.2255  0.2380  0.2151  0.2083
2       0.0723
Information flows:
 0.0431  0.0317  0.0180          0.2245 -0.6504  0.3410
-0.2711 -0.0758 -0.0277
 0.1808  0.0712  0.0127

Computing accuracy flows...
Accuracies:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.6250  0.6250  0.6010  0.6370  0.6370  0.6520  0.6540
1       0.6410  0.6490  0.6580  0.6170  0.6250  0.6500  0.6360
2       0.6420
Mutual informations:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.0456  0.0456  0.0296  0.0549  0.0549  0.0677  0.0696
1       0.0582  0.0650  0.0733  0.0399  0.0456  0.0659  0.0540
2       0.0590
Information flows:
 0.0085  0.0127  0.0508          0.0936 -0.1938  0.1083
-0.0535 -0.0304 -0.0783
 0.0357  0.0286  0.0360

-------------------------------------------------------------------------------

Results with raw info flow measures included: (n=2000)

Accuracy: 66%

Correlations with Z:
[[0.00071216 0.00193857 0.01833307]]
[[ 0.03997289 -0.00486772 -0.0346825 ]]
[[-0.00174965]]

Correlations with Y:
[[0.32444411 0.31761195 0.17291856]]
[[-0.37925824 -0.35841301  0.38317976]]
[[0.38552511]]

Weights
-1.0137 -0.6479 -1.0169         -1.4970 -1.2153  2.0050
-0.4846 -0.1121 -1.2768
 0.8699  1.0615  1.2862

Computing bias flows...
Accuracies:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.7590  0.7090  0.5260  0.7840  0.7570  0.7040  0.7750
1       0.7440  0.5820  0.7310  0.7680  0.7390  0.7540  0.7760
2       0.7110
Mutual informations:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.2033  0.1300  0.0020  0.2472  0.2000  0.1237  0.2308
1       0.1793  0.0195  0.1600  0.2185  0.1717  0.1951  0.2326
2       0.1326
Information flows:
 0.2033          0.1990          0.1326
 0.1300          0.0608
 0.0020          0.1756
Weighted information flows:
-0.2061 -0.0842 -0.0020         -0.2979 -0.0739  0.3521
-0.0985 -0.0146 -0.0025
 0.1768  0.1380  0.0025

Computing accuracy flows...
Accuracies:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.6280  0.6150  0.6020  0.6320  0.6470  0.6500  0.6580
1       0.6730  0.6470  0.6700  0.6620  0.6680  0.6650  0.6630
2       0.6670
Mutual informations:
Layer   X1      X2      X3      X12     X13     X23     X123
0       0.0478  0.0385  0.0302  0.0509  0.0633  0.0659  0.0733
1       0.0882  0.0633  0.0851  0.0771  0.0830  0.0800  0.0781
2       0.0820
Information flows:
 0.0478          0.0882          0.0820
 0.0385          0.0633
 0.0302          0.0851
Weighted information flows:
-0.0485 -0.0249 -0.0307         -0.1320 -0.0769  0.1706
-0.0232 -0.0043 -0.0386
 0.0416  0.0409  0.0389

-------------------------------------------------------------------------------

Workstation vs. Laptop timing:

Laptop:
real	3m39.118s
user	7m10.353s
sys	0m4.472s

Workstation:
real    2m52.231s
user    16m12.581s
sys     0m53.235s

